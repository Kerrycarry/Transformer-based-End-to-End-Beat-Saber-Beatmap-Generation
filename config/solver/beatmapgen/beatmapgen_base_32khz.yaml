# @package __global__

# This is the training loop solver
# for the base MusicGen model (text-to-music)
# on monophonic audio sampled at 32 kHz
defaults:
  - beatmapgen/default
  - /model: lm/musicgen_lm
  # - override /dset: audio/default
  # - override /model/lm/model_scale: xsmall
  - override /dset: audio/beatmap
  - _self_

autocast: true
autocast_dtype: float16

# EnCodec large trained on mono-channel music audio sampled at 32khz
# with a total stride of 640 leading to 50 frames/s.
# rvq.n_q=4, rvq.bins=2048, no quantization dropout
# (transformer_lm card and n_q must be compatible)
compression_model_checkpoint: //pretrained/facebook/encodec_32khz

channels: 1
sample_rate: 32000

deadlock:
  use: true  # deadlock detection

dataset:
  batch_size: 16  # 32 GPUs
  segment_duration: 256
  sample_on_weight: false  # Uniform sampling all the way
  sample_on_duration: false  # Uniform sampling all the way
  valid:
    num_samples: 400
  generate:
    num_samples: 4
  evaluate:
    num_samples: 0
  num_workers: 2


  beatmap_kwargs:
    difficulty_num: 5
    position_size: 12
    beatmap_sample_window: 16 # it must be divisble by minimum note
    minimum_note: 0.25
    minimum_bpm: 65
    maximum_bpm: 260
    note_type:  
      colorNotes: True
      chains: False 
      bombNotes: False
      obstacles: False
      arcs: False
    note_size:  
      colorNotes: 18
      chains: 2
      bombNotes: 1
      obstacles: 2
      arcs: 2


evaluate:
  metrics:
    kld: false

metrics:
  kld:
    use_gt: false
    model: passt
    passt:
      pretrained_length: 20

generate:
  every: 6
  lm:
    use_sampling: true
    top_k: 4
    top_p: 0.0
  

optim:
  epochs: 84
  updates_per_epoch: 2000
  optimizer: dadam
  lr: 0.1
  ema:
    use: true
    updates: 10
    device: cuda
  adam:
    weight_decay: 0.1

logging:
  log_tensorboard: true

schedule:
  lr_scheduler: cosine
  cosine:
    warmup: 1000
    lr_min_ratio: 1e-5
    cycle_length: 1

tensorboard:
  name: debug_test

beatmapgen_lm:
  transfer_efficient_backend: xformers # can be torch or xformers.
  blockwise_attention_kwargs:
    use_transfer_lm: True # cannot be false
    block_self_attention: True # cannot be false
    local_cross_attention: True # cannot be false
    local_self_attention: False
  ca_window_size: 0
  sa_window_size: 4 # only used if local_self_attention is True
  pad_kv: True
  use_mask: False
  lora_kwargs: 
    use_lora: False
    lora_r: 16
    lora_alpha: 16
  positional_embedding: sin
  positional_embedding_ca: null # rope or null
  positional_embedding_xy: null # rope or null
  qk_layer_norm: false
  qk_layer_norm_cross: false
  norm: layer_norm
  layer_scale: null
  use_swiglu_ffn: false
  dropout: 0.
  attention_dropout: null
  use_mask_prediction: false
  mask_schedule: [1, 1, 2, 3, 5]
  use_abstract_position: False
  token_decomposition: False
  empty_classifier_kwargs:
    use_empty_classifier: False
    alpha: 0.25
    gamma: 2
    loss_weight: 5
    simgmoid_threshold: 0.85

transformer_lm:
  lr: null # transfer learning lr
  lora_kwargs: 
    use_lora: False
    lora_r: 16
    lora_alpha: 16

audio_token:
  representation: musicgen # spectrogram, encodec or  musicgen
  encodec:
    representation_dim: 128
    target_layer: 27
    code_rate: 50
    stride_rate: 640
  musicgen: # musicgen rely on encodec param
    training_sequence_length: 1501

parser_pipeline:
  generate_url: http://localhost:8000/generate_difficulty
  generate_difficulty_version: 3  # 1,2,3,4
  generate_info_version: 2 # 2,4
  audio_duration_threshold: 1000
  nps_threshold: 1

assert_audio: False