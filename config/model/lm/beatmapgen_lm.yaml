# @package __global__

defaults:
  - lm/default
  - override /conditioner: text2music
  - override /model/lm/model_scale: small # prefer this group to set model scale instead of transformer_lm keys directly

lm_model: transformer_lm

codebooks_pattern:
  modeling: delay
  delay:
    delays: [0, 1, 2, 3]
    flatten_first: 0
    empty_initial: 0
  unroll:
    flattening: [0, 1, 2, 3]
    delays: [0, 0, 0, 0]
  music_lm:
    group_by: 2
  coarse_first:
    delays: [0, 0, 0]

transformer_lm:
  n_q: 4
  card: 2048
  memory_efficient: true
  bias_proj: false
  bias_ff: false
  bias_attn: false
  norm_first: true
  layer_scale: null
  weight_init: gaussian
  depthwise_init: current
  zero_bias_init: true
  attention_as_float32: false 

  difficulty_num: 5
  token_id_size: 18  # need + 1 in code because 18 itself is padding token
  position_size: 12
  block_pos_embeding: True
  blockwise_attention_kwargs:
    block_self_attention: True
    local_self_attention: False
    sa_window_size: 32 # attend to previous 32 notes, final window_size = 32 * 12 + 1 (+1 because attend itself)
    block_cross_attention: False
    local_cross_attention: True
    singlehead_cross_attention: False

  pad_kv: False
  # V100 need float32 instead of float16 for xformers. turn it False and set higher transfer_num_heads if using newer cards

  lora_kwargs: 
    use_lora: True
    lora_r: 16
    lora_alpha: 16
