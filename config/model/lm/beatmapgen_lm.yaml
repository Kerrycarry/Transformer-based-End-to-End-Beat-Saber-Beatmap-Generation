# @package __global__

defaults:
  - lm/default
  - override /conditioner: text2music
  - override /model/lm/model_scale: small # prefer this group to set model scale instead of transformer_lm keys directly

lm_model: transformer_lm

codebooks_pattern:
  modeling: delay
  delay:
    delays: [0, 1, 2, 3]
    flatten_first: 0
    empty_initial: 0
  unroll:
    flattening: [0, 1, 2, 3]
    delays: [0, 0, 0, 0]
  music_lm:
    group_by: 2
  coarse_first:
    delays: [0, 0, 0]

transformer_lm:
  n_q: 4
  card: 2048
  memory_efficient: true
  bias_proj: false
  bias_ff: false
  bias_attn: false
  norm_first: true
  layer_scale: null
  weight_init: gaussian
  depthwise_init: current
  zero_bias_init: true
  attention_as_float32: false 
  
  transfer_efficient_backend: xformers # can be torch or xformers.
  transfer_lr: null
  blockwise_attention_kwargs:
    use_transfer_lm: True
    block_self_attention: True
    local_self_attention: False
    sa_window_size: 32 # attend to previous 32 blocks including the block current step belong to
    block_cross_attention: False
    local_cross_attention: True
    sa_head_num: 8 # overide transfer_lm.transfer_num_heads
    ca_head_num: 8  # overide transfer_lm.transfer_num_heads

  pad_kv: True
  use_mask: False
  lora_kwargs: 
    use_lora: False
    lora_r: 16
    lora_alpha: 16

dataset:
  center: False
  beatmap_kwargs:
    difficulty_num: 5
    position_size: 12
    beatmap_sample_window: 32 # it must be divisble by minimum note
    minimum_note: 0.125
    note_type:  
      colorNotes: True
      chains: False 
      bombNotes: False
      obstacles: False
      arcs: False
    note_size:  
      colorNotes: 18
      chains: 2
      bombNotes: 1
      obstacles: 2
      arcs: 2

    